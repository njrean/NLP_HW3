{"cells":[{"cell_type":"markdown","metadata":{"id":"eB0lBzkJy_l6"},"source":["# HW3.2: Neural Transition-Based Dependency Parsing\n"]},{"cell_type":"markdown","metadata":{"id":"hwbJPnBmy_l-"},"source":["In this exercise, you are going to build a deep learning model for Neural Networks Transition-Based Dependency Parsing. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between “head” words and words which modify those heads. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time."]},{"cell_type":"markdown","metadata":{"id":"iTop7w0Wy_mT"},"source":["To complete this exercise, you will need to complete the code and build a deep learning model for dependency parsing. \n","\n","We provide the code for data preparation and the skeleton for PartialParse class. You do not need to understand the code outside of this notebook. \n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUwCUisbzOAb","executionInfo":{"status":"ok","timestamp":1679649069061,"user_tz":-420,"elapsed":10862,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"65193def-4aa6-41ef-fa6d-eb01879f5fd2"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import shutil\n","shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/HW3-2.zip\", \"/content/HW3-2.zip\")\n","!unzip -q HW3-2.zip"],"metadata":{"id":"2fJ8cYFqzPS9","executionInfo":{"status":"ok","timestamp":1679649094724,"user_tz":-420,"elapsed":25677,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb0f4d96-f65f-4ee9-d5f8-3a6cb57780bc"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["replace data/dev.conll? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","replace data/dev.gold.conll? [y]es, [n]o, [A]ll, [N]one, [r]ename: \n","error:  invalid response [{ENTER}]\n","replace data/dev.gold.conll? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"markdown","metadata":{"id":"lzF49xKny_mX"},"source":["## 1. Transition-Based Dependency Parsing"]},{"cell_type":"markdown","metadata":{"id":"PwH68mjMy_mY"},"source":["Your implementation will be a transition-based parser, which incrementally builds\n","up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows:\n","- A stack of words that are currently being processed.\n","- A buffer of words yet to be processed.\n","- A list of dependencies predicted by the parser.\n","\n","Initially, the stack only contains ROOT, the dependencies lists is empty, and the buffer contains all words\n","of the sentence in order. At each step, the parse applies a transition to the partial parse until its buffer is\n","empty and the stack is size 1. The following transitions can be applied:\n","- SHIFT: removes the first word from the buffer and pushes it onto the stack.\n","- LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of the\n","first item and removes the second item from the stack.\n","- RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second\n","item and removes the first item from the stack.\n","\n","Your parser will decide among transitions at each state using a neural network classifier."]},{"cell_type":"markdown","metadata":{"id":"Ab3kn2OVy_mb"},"source":["### TODO 1 (Written):\n","Go through the sequence of transitions needed for parsing the sentence “I parsed\n","this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the\n","configuration of the stack and buffer, as well as what transition was applied this step and what new\n","dependency was added (if any). The first three steps are provided below as an example."]},{"cell_type":"markdown","metadata":{"id":"aTkSbSf6y_ma"},"source":["Image --> https://drive.google.com/file/d/10jYgxDhsyolZGarcNTEdt6G2xB0l9iZU/view?usp=share_link "]},{"cell_type":"markdown","metadata":{"id":"OPmRQbXky_mc"},"source":["Complete the following table (double click the table and fill in the rest):\n","\n","| stack    |  buffer |  new dependency | transition |\n","| :------: |:------: | :-------------: | :--------: |\n","| \\[ROOT\\]            | \\[I, parsed, this, sentence, correctly\\] |          | Initial Configuration |\n","| \\[ROOT, I\\]         | \\[parsed, this, sentence, correctly\\]    |          | SHIFT |\n","| \\[ROOT, I, parsed\\] | \\[this, sentence, correctly\\]            |          | SHIFT |\n","| \\[ROOT, parsed\\]    | \\[this, sentence, correctly\\]            | parsed→I | LEFT-ARC |\n","| | | | |\n","| | | | |\n","| | | | |\n","| | | | |"]},{"cell_type":"markdown","metadata":{"id":"-h6PmOd6y_me"},"source":["### TODO 2 (Coding):\n","Implement the __\\_\\_init\\_\\___ and __parse_step__ functions in the PartialParse class. Your code must past both of the following tests."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXhOjsN_y_mf"},"outputs":[],"source":["class PartialParse(object):\n","    def __init__(self, sentence):\n","        \"\"\"Initializes this partial parse.\n","\n","        Your code should initialize the following fields:\n","            self.stack: The current stack represented as a list with the top of the stack as the\n","                        last element of the list.\n","            self.buffer: The current buffer represented as a list with the first item on the\n","                         buffer as the first item of the list\n","            self.dependencies: The list of dependencies produced so far. Represented as a list of\n","                    tuples where each tuple is of the form (head, dependent).\n","                    Order for this list doesn't matter.\n","\n","        The root token should be represented with the string \"ROOT\"\n","\n","        Args:\n","            sentence: The sentence to be parsed as a list of words.\n","                      Your code should not modify the sentence.\n","        \"\"\"\n","        # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.\n","        self.sentence = sentence #--list\n","\n","        ### YOUR CODE HERE\n","        #self.stack = ?  --> list\n","        #self.buffer = ? --> list\n","        #self.dependencies = ?  --> list\n","        ### END YOUR CODE\n","\n","    def parse_step(self, transition):\n","        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n","\n","        Args:\n","            transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n","                        and right-arc transitions. You can assume the provided transition is a legal\n","                        transition.\n","        \"\"\"\n","        ### YOUR CODE HERE\n","        ### END YOUR CODE\n","\n","    def parse(self, transitions):\n","        \"\"\"Applies the provided transitions to this PartialParse\n","\n","        Args:\n","            transitions: The list of transitions in the order they should be applied\n","        Returns:\n","            dependencies: The list of dependencies produced when parsing the sentence. Represented\n","                          as a list of tuples where each tuple is of the form (head, dependent)\n","        \"\"\"\n","        for transition in transitions:\n","            self.parse_step(transition)\n","        return self.dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kF_8ESPy_mk"},"outputs":[],"source":["# Do not modify this code\n","def test_step(name, transition, stack, buf, deps,\n","              ex_stack, ex_buf, ex_deps):\n","    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n","    pp = PartialParse([])\n","    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n","\n","    pp.parse_step(transition)\n","    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n","    assert stack == ex_stack, \\\n","        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n","    assert buf == ex_buf, \\\n","        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n","    assert deps == ex_deps, \\\n","        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n","    print(\"{:} test passed!\".format(name))\n","\n","\n","def test_parse_step():\n","    \"\"\"Simple tests for the PartialParse.parse_step function\n","    Warning: these are not exhaustive\n","    \"\"\"\n","    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n","              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n","    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n","              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n","    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n","              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n","\n","\n","def test_parse():\n","    \"\"\"Simple tests for the PartialParse.parse function\n","    Warning: these are not exhaustive\n","    \"\"\"\n","    sentence = [\"parse\", \"this\", \"sentence\"]\n","    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n","    dependencies = tuple(sorted(dependencies))\n","    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n","    assert dependencies == expected,  \\\n","        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n","    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n","        \"parse test failed: the input sentence should not be modified\"\n","    print(\"parse test passed!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwNZjFgey_mn"},"outputs":[],"source":["test_parse_step()\n","test_parse()"]},{"cell_type":"markdown","metadata":{"id":"EL5kIWKXy_m4"},"source":["## 2. Setup and Preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zemNby1uy_m5","executionInfo":{"status":"ok","timestamp":1679637678758,"user_tz":-420,"elapsed":2,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}}},"outputs":[],"source":["from utils.parser_utils import minibatches, load_and_preprocess_data"]},{"cell_type":"markdown","metadata":{"id":"RksEEdJvy_m8"},"source":["Preparing data. We will use a subset of Penn Treebank and pretrained embeddings in this task"]},{"cell_type":"markdown","metadata":{"id":"5XNgGpMUy_m9"},"source":["We are now going to train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next. First, the model extracts a feature vector representing the current state. We will be using the feature set presented in the original neural dependency parsing paper: A Fast and Accurate Dependency Parser using Neural Networks. \n","\n","The function extracting these features has been implemented for you in parser_utils. This feature vector consists of a list of tokens (e.g., the last word in the stack, first word in the buffer, dependent of the second-to-last word in the stack if there is one, etc.). They can be represented as a list of integers."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhqbGTYpy_m-","executionInfo":{"status":"ok","timestamp":1679637686543,"user_tz":-420,"elapsed":5752,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"c67f8096-2c13-4e42-b52c-ae2521f5d45e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","took 2.40 seconds\n","Building parser...\n","took 0.03 seconds\n","Loading pretrained embeddings...\n","took 2.38 seconds\n","Vectorizing data...\n","took 0.06 seconds\n","Preprocessing training data...\n","took 1.17 seconds\n"]}],"source":["parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(True)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-VYL2rKy_nB","executionInfo":{"status":"ok","timestamp":1679637686543,"user_tz":-420,"elapsed":17,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"20a722e9-44e1-4c71-a820-30ab47e60820"},"outputs":[{"output_type":"stream","name":"stdout","text":["48390 500 500\n"]}],"source":["print(len(train_examples), len(dev_set), len(test_set))"]},{"cell_type":"code","source":["train_examples[10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cUGa5-KldWCD","executionInfo":{"status":"ok","timestamp":1679637686544,"user_tz":-420,"elapsed":14,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"68039d8f-94b4-4279-8c12-4ebe091c42a6"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([5156,\n","  660,\n","  88,\n","  96,\n","  85,\n","  2131,\n","  5155,\n","  5155,\n","  5155,\n","  5155,\n","  5155,\n","  5155,\n","  91,\n","  5155,\n","  113,\n","  5155,\n","  5155,\n","  5155,\n","  84,\n","  39,\n","  40,\n","  61,\n","  41,\n","  39,\n","  83,\n","  83,\n","  83,\n","  83,\n","  83,\n","  83,\n","  40,\n","  83,\n","  41,\n","  83,\n","  83,\n","  83],\n"," [1, 1, 1],\n"," 2)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["embeddings"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yo_qz2-11MvG","executionInfo":{"status":"ok","timestamp":1679637686544,"user_tz":-420,"elapsed":12,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"776511de-28c0-4d36-ea01-274b5f9b6825"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.7292381 , -0.33759412,  0.33548045, ...,  0.5354017 ,\n","        -0.20434363,  1.0421315 ],\n","       [-0.4500417 , -1.6892221 ,  0.4156098 , ...,  0.35154536,\n","        -0.23606075, -2.0565262 ],\n","       [-0.8837722 , -0.53987306, -0.90973115, ...,  0.21462597,\n","        -0.12736242,  2.060031  ],\n","       ...,\n","       [ 0.6028972 ,  0.05559519,  0.4461097 , ..., -0.5107466 ,\n","         0.43016586,  1.3869425 ],\n","       [-0.43125707, -0.30145568, -0.8677901 , ..., -0.23169452,\n","        -0.506528  ,  0.6272445 ],\n","       [ 0.39807355,  1.7552437 ,  0.4275137 , ..., -0.21109965,\n","         1.078214  ,  0.13150327]], dtype=float32)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FubWDIsNy_nE","executionInfo":{"status":"ok","timestamp":1679637686544,"user_tz":-420,"elapsed":10,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"9a229b71-f9d4-473d-b928-3f7fb2a1e4d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["(5157, 50)\n"]}],"source":["print(embeddings.shape)"]},{"cell_type":"markdown","metadata":{"id":"ZBD3A4yVy_nI"},"source":["Get the full batch of our subset data"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"8vNwux9by_nJ","executionInfo":{"status":"ok","timestamp":1679637686545,"user_tz":-420,"elapsed":9,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}}},"outputs":[],"source":["minibatch_gen = minibatches(train_examples, len(train_examples))\n","x_train, y_train = minibatch_gen.__next__()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i__8jliGy_nM","executionInfo":{"status":"ok","timestamp":1679637686545,"user_tz":-420,"elapsed":8,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"74ed1a72-4183-4494-972f-7e93f456f516"},"outputs":[{"output_type":"stream","name":"stdout","text":["(48390, 36)\n","(48390, 3)\n"]}],"source":["print(x_train.shape)\n","print(y_train.shape)"]},{"cell_type":"code","source":["x_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ls7ApWjlNgOz","executionInfo":{"status":"ok","timestamp":1679637745105,"user_tz":-420,"elapsed":4,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"3624500c-a7d4-46c7-e5ac-0bd54a71b08b"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  88,  241,  245,  339,   87, 5155, 5155, 5155, 5155, 5155, 5155,\n","       5155, 5155, 5155, 5155, 5155, 5155, 5155,   40,   70,   44,   57,\n","         46,   83,   83,   83,   83,   83,   83,   83,   83,   83,   83,\n","         83,   83,   83])"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["You can use parser.id2tok[word_id] to lookup the word in English."],"metadata":{"id":"LbVsKG-rOFdp"}},{"cell_type":"code","source":["for word_id in x_train[0]:\n","  print(parser.id2tok[word_id])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_vlptDWNEs_","executionInfo":{"status":"ok","timestamp":1679637860711,"user_tz":-420,"elapsed":3,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"6c2766be-8dee-4b20-8764-a9de31c3e09e"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["of\n","how\n","computers\n","work\n",".\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<p>:IN\n","<p>:WRB\n","<p>:NNS\n","<p>:VBP\n","<p>:.\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n"]}]},{"cell_type":"code","source":["y_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEJeDWvkclRS","executionInfo":{"status":"ok","timestamp":1679591605715,"user_tz":-420,"elapsed":9,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"}},"outputId":"e4b263e7-a508-4338-ed65-8803ac0635f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 1., 0.])"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"17eqeESxy_nR"},"source":["## 3. Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5mqcz1qy_nT"},"outputs":[],"source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense, Reshape, Dropout, Flatten\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{"id":"5sW29Rhvy_nX"},"source":["### TODO 3 (Coding):\n","Build and train a tensroflow keras model to predict an action for each state of of the input. This is a simple classification task. \n","- The input and output of the model must match the dimention of x_train and y_train.\n","- The model must use the provided pretrained embeddings\n","- The model could comprise of only a feedforward layer and a dropout\n","- Training loss should be around 0.1 or below, and training categorical_accuracy above 0.94"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAWlQI6iy_nY"},"outputs":[],"source":["model = Sequential()\n","# Write your code here\n","pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMjs0W69y_nb"},"outputs":[],"source":["# Write your code here\n","model.fit()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}